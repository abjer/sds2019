{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 7: Data structuring  3\n",
    "\n",
    "*Morning, August 15, 2019*\n",
    "\n",
    "In this Exercise Set we finalize our work with the weather data we started working on in Exercise Set 4. We will also study a dataset of traffic data from Copenhagen to iterate through the pandas workflow once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Increases the plot size a little\n",
    "mpl.rcParams['figure.figsize'] = 11, 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.1: Weather data, part 3\n",
    "We continue with the final part of three exercises on structuring weather data. In this exercise you must use the function for fetching and structuring weather data which you made in Exercise 6.1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.1:** Plot the monthly max,min, mean, first and third quartiles for maximum temperature for our station with the ID _'ITE00100550'_ in 1864. \n",
    "\n",
    "> *Hint*: the method `describe` computes all these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.1]\n",
    "\n",
    "# This will be in assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.2:** Get the processed data from years 1864-1867 as a list of DataFrames. Convert the list into a single DataFrame by concatenating vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.2]\n",
    "\n",
    "# This will be in assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.3:** Parse the station location data which you can find at https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt. Merge station locations onto the weather data spanning 1864-1867.  \n",
    "\n",
    "> _Hint:_ The location data have the folllowing format, \n",
    "\n",
    "```\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "> *Hint*: The station information has fixed width format - does there exist a pandas reader for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.3]\n",
    "\n",
    "# This will be in assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.2: Traffic data in Copenhagen\n",
    "\n",
    "In this second part of exercise set 7 you will be working with traffic data from Copenhagen Municipality.\n",
    "\n",
    "The municipality have made the data openly available through the [opendata.dk](http://www.opendata.dk/) platform. We will use the data from traffic counters to construct a dataset of hourly traffic. We will use this data to get basic insights on the development in traffic over time and relate it to weather. The gist here is to practice a very important skill in Data Science: being able to quickly fetch data from the web and structure it so that you can work with it. Scraping usually gets a bit more advanced than what we will do today, but the following exercises should give you a taste for how it works. The bulk of these exercise, however, revolve around using the Pandas library to structure and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7.2.a: getting some data to work with\n",
    "\n",
    "Hence follows a simple scraping exercise where you (1) collect urls for datasets in the webpage listing data on traffic counters and (2) use these urls to load the data into one dataframe.\n",
    "\n",
    "> **Ex. 7.2.1:** Using the requests module, extract the html markup of the webpage data.kk.dk/dataset/faste-trafiktaellinger and store it as a string in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.1]\n",
    "\n",
    "from requests import get \n",
    "\n",
    "url = 'https://data.kk.dk/dataset/faste-trafiktaellinger'\n",
    "resp = get(url)\n",
    "\n",
    "html = resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.2:** Using the re module, extract a list of all the urls in the html string and store them in a new variable.\n",
    "\n",
    "> _Hint:_ Try using the re.findall method. You may want to Google around to figure out how to do this. Protip: searching for something along the lines of \"extract all links in html regex python\" and hitting the first StackOverflow link will probably get you farther than reading elaborate documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.2]\n",
    "\n",
    "import re \n",
    "\n",
    "rehits = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.3:** Create a new variable that only contains the links that point to downloadable traffic data sheets. \n",
    "\n",
    "> _Hint:_ You want to filter the results from above. For example to only include urls with the term 'download' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.3]\n",
    "\n",
    "datalinks = [url for url in rehits if 'download' in url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.4:** Using pd.read_excel method, load the datasets into a list. Your resulting variable should hold a list of Pandas dataframes.\n",
    "\n",
    "> _Hint:_ you may want to set the skiprows= keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.4]\n",
    "\n",
    "data = [pd.read_excel(url, skiprows = 10) for url in datalinks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.5:** Merge the list of dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.5]\n",
    "\n",
    "final_data = pd.concat(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7.2.b Structuring your data\n",
    "\n",
    "If you successfully completed the previous part, you should now have a dataframe with about 183.397 rows (if your number of rows is close but not the same, worry not—it matters little in the following). Well done! But the data is still in no shape for analysis, so we must clean it up a little.\n",
    "\n",
    "161.236 rows (and 30 columns) is a lot of data. ~3.3 MB by my back-of-the-envelope calculations, so not \"Big Data\", but still enough to make your CPU heat up if you don't use it carefully. Pandas is built to handle fairly large dataframes and has advanced functionality to perform very fast operations even when the size of your data grows huge. So instead of working with basic Python we recommend working pandas built-in procedures as they are constructed to be fast on dataframes.\n",
    "\n",
    "Nerd fact: the reason pandas is much faster than pure Python is that dataframes access a lower level programming languages (namely C, C++) which are multiple times faster than Python. The reason it is faster is that it has a higher level of explicitness and thus is more difficult to learn and navigate.\n",
    "\n",
    "> **Ex. 7.2.6:** Reset the row indices of your dataframe so the first index is 0 and the last is whatever the number of rows your dataframe has. Also drop the column named 'index' and the one named `Spor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.6]\n",
    "\n",
    "final_data = final_data\\\n",
    "        .reset_index()\\\n",
    "        .drop(['index', 'Spor'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.7:** Rename variables from Danish to English using the dictionary below.\n",
    "\n",
    "```python \n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.7]\n",
    "\n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "\n",
    "\n",
    "final_data = final_data.rename(columns = dk_to_uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is quite efficient. For example, when you create a new dataframe by manipulating an old one, Python notices that—apart from some minor changes—these two objects are almost the same. Since memory is a precious resource, Python will represent the values in the new dataframe as references to the variables in the old dataset. This is great for performance, but if you for whatever reason change some of the values in your old dataframe, values in the new one will also change—and we don't want that! Luckily, we can break this dependency.\n",
    "\n",
    "> **Ex. 7.2.8:** Break the dependencies of the dataframe that resulted from Ex. 7.2.7 using the `.copy` method. Delete all other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.8]\n",
    "\n",
    "new_data = final_data.copy()\n",
    "\n",
    "del final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have structured appropriately, something that you will want to do again and again is selecting subsets of the data. Specifically, it means that you select specific rows in the dataset based on some column values.\n",
    "\n",
    ">**Ex. 7.2.9:** Create a new column in the dataframe called total that is True when the last letter of road_id is T and otherwise False.\n",
    "\n",
    "> _Hint:_ you will need the `pd.Series.str` attribute for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.9]\n",
    "\n",
    "new_data['total'] = (new_data.road_id.str[-1] == 'T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.10:** Select rows where total is True. Delete all the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.10]\n",
    "\n",
    "total_data = new_data[new_data.total == True]\n",
    "\n",
    "del new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.11:** Make two datasets based on the lists of columns below. Call the dataset with spatial columns data_geo and the other data.\n",
    "\n",
    "```python\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = ['kl.{}-{}'.format(str(h).zfill(2), str(h+1).zfill(2)) for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.11]\n",
    "\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = ['kl.{}-{}'.format(str(h).zfill(2), str(h+1).zfill(2)) for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "\n",
    "# selections\n",
    "data_geo = total_data[spatial_columns]\n",
    "data  = total_data[select_columns]\n",
    "\n",
    "del total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.12:** Drop the duplicate rows in data_geo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.12]\n",
    "\n",
    "data_geo.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting: wide and narrow format**\n",
    "\n",
    "When talking about two-dimensional data (matrices, tables or dataframes, we can call it many things), we can either say that it is in wide or long format (see explanation here, \"wide\" and \"long\" are used interchangably). In Pandas we can use the commands stack and unstack to move between these formats.\n",
    "\n",
    "The wide format has the advantage that it often requires less storage and is easier to read when printed. On the other hand the long format can be easier for modelling, because each observation has its own row. Turns out that the latter is what we most often need.\n",
    "\n",
    "> **Ex. 7.2.13:** Turn the dataset from wide to long so hourly data is now vertically stacked. Store this dataset in a dataframe called data. Name the column with hourly information hour_period. Your resulting dataframe should look something like this.\n",
    "\n",
    "> _Hint:_ pandas' melt function may be of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.13]\n",
    "\n",
    "# A bonus info: This function is written using optional type declaration. This\n",
    "# is used to communicate what type the inputs should be. It doesn't affect the \n",
    "# functionality in any way, but it can sometimes increase readability. \n",
    "\n",
    "def gather(df: pd.DataFrame, key: str, value: str, cols: list) -> pd.DataFrame:\n",
    "    \"\"\" This helper makes pythons melt() behave like R's gather()\n",
    "    \"\"\"\n",
    "    id_vars = [ col for col in df.columns if col not in cols ]\n",
    "    id_values = cols\n",
    "    var_name = key\n",
    "    value_name = value\n",
    "    return pd.melt( df, id_vars, id_values, var_name, value_name )\n",
    "\n",
    "data_long = gather(data, 'hour_period', 'value', hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical data**\n",
    "\n",
    "Categorical data can contain Python objects, usually strings. These are smart if you have variables with string observations that are long and often repeated, e.g. with road names.\n",
    "\n",
    "> **Ex. 7.2.14:** Use the `.astype` method to convert the type of the road_name column to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.14]\n",
    "\n",
    "data_long.road_name = data_long.road_name.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure temporal data\n",
    "\n",
    "Pandas has native support for working with temporal data. This is handy as much 'big data' often has time stamps which we can make Pandas aware of. Once we have encoded temporal data it can be used to extract information such as the hour, second etc.\n",
    "\n",
    "> **Ex. 7.2.15:** Create a new column called hour which contains the hour-of-day for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.15]\n",
    "\n",
    "data_long['hour'] = pd.to_datetime(data_long['hour_period'].str[3:5], format = '%H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.2.16:** Create a new column called time, that contains the time of the row in datetime format. Delete the old temporal columns (hour, hour_period, date) to save memory.\n",
    "\n",
    "> _Hint:_ try making an intermediary series of strings that has all temporal information for the row; then use pandas to_datetime function where you can specify the format of the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.16]\n",
    "\n",
    "data_long['timestamp'] = pd.to_datetime(data_long.date + ' ' + data_long\\\n",
    "                                        .hour\\\n",
    "                                        .dt\\\n",
    "                                        .hour\\\n",
    "                                        .astype(str)\\\n",
    "                                        .apply(lambda x: x.zfill(2)),            \n",
    "                                        format = '%d.%m.%Y %H')\n",
    "\n",
    "data_long = data_long.drop(['hour', 'hour_period', 'date'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.17:** Using your time column make a new column called weekday which stores the weekday (in values between 0 and 6) of the corresponding datetime.\n",
    "\n",
    "> _Hint:_ try using the dt method for the series called time; dt has some relevant methods itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.17]\n",
    "\n",
    "data_long['weekday'] = data_long.timestamp.dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical descriptions of traffic data\n",
    "\n",
    "> **Ex. 7.2.18:** Print the \"descriptive statistics\" of the traffic column. Also show a kernel density estimate of the values.\n",
    "\n",
    "> _Hint:_ Use the describe method of pandas dataframes for the first task. Use seaborn for the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.18]\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "print(data_long.value.describe())\n",
    "\n",
    "sb.kdeplot(data_long.value.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.19:** Which road has the most average traffic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.19]\n",
    "\n",
    "data_long.groupby('road_name')\\\n",
    "         .value.agg('mean')\\\n",
    "         .reset_index()\\\n",
    "         .sort_values('value', ascending = False)\\\n",
    "         .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.20:** Compute annual, average road traffic during day hours (9-17). Which station had the least traffic in 2013? Which station has seen highest growth in traffic from 2013 to 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.19]\n",
    "\n",
    "# create year variable\n",
    "data_long['year'] = data_long.timestamp.dt.year\n",
    "data_long['hour'] = data_long.timestamp.dt.hour\n",
    "\n",
    "# subset relevant years and times \n",
    "data_delta = data_long.query(\"year == 2013 | year == 2014\")\\\n",
    "                      .query(\"hour >= 9 & hour <= 17\")\n",
    "\n",
    "# convert to wide format\n",
    "data_delta = data_delta[['road_name', 'value', 'year']].groupby(['road_name', 'year'])\\\n",
    "                            .agg('mean')\\\n",
    "                            .reset_index()\\\n",
    "                            .pivot('road_name', 'year', 'value')\\\n",
    "                            .reset_index()\n",
    "\n",
    "# calculate relative size\n",
    "data_delta['growth'] = (data_delta[2014]/data_delta[2013] - 1)*100\n",
    "\n",
    "# show\n",
    "data_delta.sort_values('growth', ascending = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
